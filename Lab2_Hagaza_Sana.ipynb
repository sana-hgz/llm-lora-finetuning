{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8Z2srf-7VTu"
      },
      "source": [
        "## <b>Instruction Finetuning</b>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tzv7XNDM7Tim"
      },
      "source": [
        "\n",
        "\n",
        "In this lab, you will learn about fine-tuning large language models (LLMs) for specific tasks.\n",
        "\n",
        "Instruction fine-tuning enables models to follow human instructions effectively by training on high-quality instruction-response pairs.\n",
        "\n",
        "We will implement these techniques using Python and the Hugging Face ecosystem, including transformers and datasets,  By the end of the lab, you will have hands-on experience in adapting LLMs to specific use cases and evaluating their performance.\n",
        "\n",
        "In summary, we will:\n",
        "\n",
        "* Finetune [Qwen2-0.5B](https://huggingface.co/Qwen/Qwen2-0.5B) on a question/answer dataset.\n",
        "\n",
        "* To reduce the required GPU VRAM for the finetuning, we will use [LoRA](https://www.anyscale.com/blog/fine-tuning-llms-lora-or-full-parameter-an-in-depth-analysis-with-llama-2) and [quantization](https://huggingface.co/blog/4bit-transformers-bitsandbytes) techniques.\n",
        "\n",
        "* Compare the results before and after instruction tuning.\n",
        "\n",
        "<center>\n",
        "<img src='https://onedrive.live.com/embed?resid=AE69638675180117%21292802&authkey=%21AO_qaECmI1InIyg&width=634&height=556' width=\"500\">\n",
        "\n",
        "\n",
        "LoRA: Low Rank Adapataion. Taken from LoRA original paper\n",
        "\n",
        "<img src='https://onedrive.live.com/embed?resid=AE69638675180117%21292801&authkey=%21AIBM2HNKRF7tzGo&width=1980&height=866' width=\"700\">\n",
        "\n",
        "QLoRA. Taken from QLoRA original paper\n",
        "\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdl5minU7JhQ"
      },
      "source": [
        "### <b>Finetuning Qwen2.5-0.5B using HuggingFace's Transfromers</b>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUCV4V0ONKeJ"
      },
      "source": [
        "\n",
        "In this section, we will fintune [Qwen2.5-0.5B](https://huggingface.co/Qwen/Qwen2.5-0.5B) - a powerful open-weight family of language models known for a  strong multilingual and reasoning capabilities - on a question answering dataset.\n",
        "\n",
        "Supervised Fine-Tuning (SFT) is a crucial step in adapting pre-trained language models to specific tasks or domains by training them on high-quality instruction-response pairs. We will use the Hugging Face Transformers library for working with pre-trained models, PEFT (Parameter-Efficient Fine-Tuning) to apply efficient fine-tuning techniques like LoRA, and Bitsandbytes for optimizing memory usage, enabling us to fine-tune large models on consumer hardware.\n",
        "\n",
        "A key aspect of fine-tuning conversational models is structuring prompts correctly using chat templates. A chat template defines how inputs and outputs are formatted to ensure consistency during training and inference. In our lab, we will use the following chat template:\n",
        "```\n",
        "<human>: {Question}\n",
        "<assistant>: {Answer}\n",
        "```\n",
        "\n",
        "Such formats helps the model differentiate between user inputs and assistant responses, ensuring better alignment with real-world chat applications.\n",
        "\n",
        "In this section, we will focus on completion-only fine-tuning, meaning we will train the model only on generating the assistant’s response while not learning to generate the prompt. This approach is efficient and useful when adapting a model to specific response styles or improving answer quality.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Kk-AEdr65TO"
      },
      "source": [
        "#### <b>Preparing the environment and installing libraries:<b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvYPeqtmLTiu",
        "outputId": "fc5d41ea-c4cc-4988-c0b4-0c490500b783"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sun Oct 12 15:03:39 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   74C    P0             33W /   70W |   14676MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "khRdXTxqy9V_"
      },
      "outputs": [],
      "source": [
        "!pip install -qqq bitsandbytes torch transformers peft accelerate datasets loralib einops trl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "qHHXf0xHUsx9"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from pprint import pprint\n",
        "\n",
        "import bitsandbytes as bnb\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import transformers\n",
        "from datasets import load_dataset\n",
        "from trl import DPOConfig, DPOTrainer\n",
        "\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    PeftConfig,\n",
        "    PeftModel,\n",
        "    get_peft_model,\n",
        "    prepare_model_for_kbit_training,\n",
        "    PeftModelForCausalLM\n",
        ")\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzHeSpov7CCZ"
      },
      "source": [
        "#### <b>Loading the model and the tokenizer:</b>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MC-Kv8g8MSuW"
      },
      "source": [
        "\n",
        "\n",
        "In this section, we will load the Qwen model while using the BitsAndBytes library for quantization.\n",
        "\n",
        "The Bitsandbytes library is a powerful tool for optimizing large language model (LLM) training and inference by enabling 8-bit and 4-bit quantization, significantly reducing memory usage while maintaining model performance. Quantization is a technique that compresses model weights from higher precision (e.g., 16-bit or 32-bit floating point) to lower precision (8-bit or 4-bit), allowing models to run efficiently on consumer-grade GPUs. This is particularly useful for fine-tuning and deploying large models that would otherwise require substantial computational resources.\n",
        "\n",
        "In Bitsandbytes, key parameters control how quantization is applied:\n",
        "\n",
        "- **nf4 (Normalized Float 4)**: A 4-bit data type designed to better preserve model accuracy by focusing on commonly used weight ranges.\n",
        "- **bnb_4bit_compute_dtype**.\n",
        "- **bnb_4bit_quant_type**: Specifies the quantization method, commonly \"nf4\" or \"fp4\" (floating-point 4-bit).\n",
        "- **load_in_4bit=True**: Enables 4-bit quantization for efficient memory usage.\n",
        "- **load_in_8bit=True**: Enables 8-bit quantization, which offers a trade-off between efficiency and precision.\n",
        "- **bnb_4bit_use_double_quant**.\n",
        "\n",
        "Quantization works by mapping continuous weight values into a smaller discrete range, which reduces the memory footprint of the model while keeping it functionally effective. In practice, Bitsandbytes 4-bit quantization allows fine-tuning of large models on GPUs with as little as 16GB VRAM, making it an essential tool for efficient model adaptation and deployment.\n",
        "\n",
        "In our lab, we will store the model in the VRAM with 4 bits using the 'nf4' quantization method, do the computation using brain float 16 (BF16) and use double quantization.\n",
        "\n",
        "<b><h4><font color='red'>\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "Question 1: </b><br>\n",
        "What is computation dtype in the context of quantization (which can be specified using bnb_4bit_compute_dtype)? What is the importance of double quantization?\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0U4t0WAB0hJS"
      },
      "source": [
        "<b><h4><font color='green'>\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "Answer 1: </b><br>\n",
        "\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53PIMaW700h1"
      },
      "source": [
        "Computation dtype (bnb_4bit_compute_dtype) : This specifies the data type used for actual computations during forward and backward passes, even though the model weights are stored in 4-bit format. While weights are kept in memory as 4-bit to save space, they are temporarily dequantized to a higher precision format (like BF16 or FP16) for matrix multiplications and other operations. This is crucial because:\n",
        "\n",
        "* 4-bit precision is too low for stable gradient computations\n",
        "* BF16 provides a good balance between memory efficiency and numerical stability\n",
        "* It prevents accumulation of numerical errors during training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q32iROrw8Dpt"
      },
      "source": [
        "Double Quantization : This technique applies quantization twice - first to the model weights, then to the quantization constants themselves. Normally, quantization requires storing scaling factors and zero points in higher precision (FP32). Double quantization also quantizes these constants which allows reducing memory overhead. This provides additional memory savings with negligible impact on model quality.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqOLPM9R0qDd"
      },
      "source": [
        "<b><h4><font color='blue'>\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "Task 1: </b><br>\n",
        "According to what is described earlier, fill the gap to create our BitsAndBytes configuration.\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "X6TaXDnRVKDq"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"Qwen/Qwen2.5-0.5B\"\n",
        "# MODEL_NAME = \"unsloth/Llama-3.2-1B\" # to go further, try llama with unsloth\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(load_in_4bit=True,\n",
        "                                bnb_4bit_quant_type= 'nf4',\n",
        "                                bnb_4bit_compute_dtype = torch.bfloat16,\n",
        "                                bnb_4bit_use_double_quant = True) ## FILL THE GAP: configuration of quantization\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    quantization_config=bnb_config,\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTgKyxhJMeEP"
      },
      "source": [
        "#### <b>Configuring LoRA:</b>\n",
        "\n",
        "PEFT (Parameter-Efficient Fine-Tuning) is a library designed to fine-tune large language models (LLMs) efficiently by updating only a small subset of parameters, instead of the entire model. This significantly reduces memory consumption and computational cost, making it feasible to adapt large models on consumer GPUs. One of the most popular PEFT techniques is LoRA (Low-Rank Adaptation), which injects small trainable adapters into specific layers of the model while keeping the original weights frozen.\n",
        "\n",
        "Instead of modifying the large pre-trained weight matrices directly, **LoRA** decomposes weight updates into two smaller matrices of a lower rank. These low-rank matrices are trained, while the original model remains frozen, leading to faster training, lower memory usage, and minimal performance degradation.\n",
        "\n",
        "When applying LoRA using PEFT, several important parameters are used:\n",
        "\n",
        "- **r (Rank)**: The rank of the low-rank matrices added to the model.\n",
        "Common Practice: Values like 8, 16, or 32 are often used. Higher ranks improve model adaptability but require more memory. In our lab we will use a LoRA rank of 32.\n",
        "- **lora_alpha**: The scaling factor for LoRA updates.\n",
        "Common Practice: Set as 2 × rank (e.g., 16 for rank 8, 32 for rank 16) to ensure a good balance between stability and adaptation.\n",
        "- **lora_dropout**: Dropout applied to LoRA layers to prevent overfitting.\n",
        "Common Practice: 0.05–0.1 is commonly used. In our lb we will use 0.05.\n",
        "- **target_modules**: Specifies which model layers should be fine-tuned with LoRA.\n",
        "Common Practice: For transformer models like LLaMA, Qwen, and Mistral, LoRA is typically applied on all projection (MLP) layers inside the transformer block (so excluding the embedding and language modeling head layers).\n",
        "\n",
        " **Note:** set `bias` to `'none'` and do not forget to set the `task_type` to the causla language modeling task.\n",
        "\n",
        "<b><h4><font color='blue'>\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "Task 2: </b><br>\n",
        "Fill the gap in the next cell to compute the number of trainable parameters in a pytorch model in order to check later the effect of using LoRA. <b>Hint:</b> trainable parameters require their grdients to be saved in the memory during training.\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "LIvuxW4lVW_E"
      },
      "outputs": [],
      "source": [
        "def print_trainable_parameters(model):\n",
        "\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        ## FILL THE GAP: get the number of trainable parameters: trainable_params\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tr9lJtcRbau9",
        "outputId": "323a8bfd-a144-405d-9a91-6a8e771d064e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 136178560 || all params: 315119488 || trainable%: 43.21489631260127\n"
          ]
        }
      ],
      "source": [
        "print_trainable_parameters(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nywupL1V_Y1f"
      },
      "source": [
        "<b><h4><font color='blue'>\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "Task 3: </b><br>\n",
        "According to what is described earlier, fill the gap to create your LoRA configuration then use it to define your model. <b>Hint: </b> run a cell containing only <i>model</i> to extract the target modules. <b>\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NeyjCtqLp6t",
        "outputId": "944c3815-a263-4c48-abbd-20117dd8f480"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Qwen2ForCausalLM(\n",
              "  (model): Qwen2Model(\n",
              "    (embed_tokens): Embedding(151936, 896)\n",
              "    (layers): ModuleList(\n",
              "      (0-23): 24 x Qwen2DecoderLayer(\n",
              "        (self_attn): Qwen2Attention(\n",
              "          (q_proj): Linear4bit(in_features=896, out_features=896, bias=True)\n",
              "          (k_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
              "          (v_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
              "          (o_proj): Linear4bit(in_features=896, out_features=896, bias=False)\n",
              "        )\n",
              "        (mlp): Qwen2MLP(\n",
              "          (gate_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=4864, out_features=896, bias=False)\n",
              "          (act_fn): SiLUActivation()\n",
              "        )\n",
              "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "      )\n",
              "    )\n",
              "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "    (rotary_emb): Qwen2RotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duTYSKKYVamH",
        "outputId": "68240dd0-95f9-4db1-de0f-2a713e56eb09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 17596416 || all params: 332715904 || trainable%: 5.288721034507566\n"
          ]
        }
      ],
      "source": [
        "config = LoraConfig(r = 32,\n",
        "                    lora_alpha=64,\n",
        "                    lora_dropout = 0.05,\n",
        "                    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "                    bias = 'none',\n",
        "                    task_type=\"CAUSAL_LM\") ## FILL THE GAP: Configuration of LoRA\n",
        "\n",
        "model = get_peft_model(model, config) ## FILL THE GAP: define the model using LoRA configs\n",
        "print_trainable_parameters(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hG1vTbsx_p5Y"
      },
      "source": [
        "<b><h4><font color='red'>\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "Question 2: </b><br>\n",
        "With a small language model of 0.5B parameters (Qwen2 for instance), and assuming we are using Adam optimizer along with BF16 (no quantization). Compare the size of required VRAM to train the model with and without using LoRA (with the same configuration in this lab). Please detail you answer (i.e. required VRAM for model parameters, gradients and optimizer states. <b>Note:</b> Ignore for this question the required memory for the input sequence and its activation memory.\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtUCDSN6_6Ah"
      },
      "source": [
        "<b><h4><font color='green'>\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "Answer 2: </b><br>\n",
        "\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ccljmti5fz_3"
      },
      "source": [
        "###Without LoRA (Standard Fine-tuning)\n",
        "Training 216.05M parameters (43.21%):\n",
        "1. Model Parameters (BF16):\n",
        "\n",
        "All parameters: 500M × 2 bytes = 1,000 MB = 1.0 GB\n",
        "\n",
        "2. Gradients (BF16) - Trainable only:\n",
        "\n",
        "0.4321 × 500M × 2 bytes = 432.1 MB = 0.432 GB\n",
        "\n",
        "3. Optimizer States (Adam - BF16) - Trainable only:\n",
        "\n",
        "First moment: 216.05M × 2 bytes = 432.1 MB\n",
        "\n",
        "Second moment: 216.05M × 2 bytes = 432.1 MB\n",
        "\n",
        "Total optimizer states: 846,2 MB = 0.846 GB\n",
        "\n",
        "\n",
        "Total VRAM (Without LoRA): 1 + 0.432 + 0.846 = 2.28 GB\n",
        "\n",
        "###With LoRA (Parameter-Efficient Fine-tuning)\n",
        "Training only 26.45M LoRA parameters (5.29%):\n",
        "1. Model Parameters (BF16):\n",
        "\n",
        "Base model: 500M × 2 bytes = 1,000 MB = 1.0 GB\n",
        "LoRA adapters: 26.45M × 2 bytes = 52.9 MB\n",
        "Total: 1.053 GB\n",
        "\n",
        "2. Gradients (BF16) - LoRA only:\n",
        "\n",
        "26.45M × 2 bytes = 52.9 MB = 0.053 GB\n",
        "\n",
        "3. Optimizer States (Adam - BF16) - LoRA only:\n",
        "\n",
        "First moment: 26.45M × 2 bytes = 52.9 MB\n",
        "\n",
        "Second moment: 26.45M × 2 bytes = 52.9 MB\n",
        "\n",
        "Total optimizer states: 105.8 MB = 0.105 GB\n",
        "\n",
        "Total VRAM (With LoRA): 1 + 0.053 + 0.105 =  1.158 GB ≈ 1.16 GB\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5H1bBQaSNVsr"
      },
      "source": [
        "#### <b>Test the model before finetuning:</b>\n",
        "\n",
        "A chat template defines how inputs and responses are formatted when interacting with a conversational model. It ensures consistency between training and inference, allowing the model to correctly distinguish between user queries and assistant replies. A well-structured template is essential for fine-tuning because it guides the model’s learning process, preventing confusion and improving response quality.\n",
        "\n",
        "As mentioned before, in this lab, we will use the following chat template:\n",
        "\n",
        "```\n",
        "<human>: {Question}\n",
        "<assistant>: {Answer}\n",
        "```\n",
        "\n",
        "\n",
        "<b><h4><font color='blue'>\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "Task 4: </b><br>\n",
        "Fill the gap to create a simple prompt using the described chat template with the question: <i>What equipment do I need for rock climbing?</i> Then test what the model generate before finetuning.\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uj3ZrYkKqb0M"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRW7HPX6WCmI",
        "outputId": "3018ff3a-64d3-457e-c9a6-40e86611a986"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<human>: What equipment do I need for rock climbing?\n",
            "<assistant>:\n"
          ]
        }
      ],
      "source": [
        "prompt = prompt = \"<human>: What equipment do I need for rock climbing?\\n<assistant>:\"  ## FILL THE GAP: construct the promp with an empty response from the assistant\n",
        "print(prompt)\n",
        "\n",
        "generation_config = model.generation_config\n",
        "generation_config.max_new_tokens = 200\n",
        "generation_config.temperature = 0.7\n",
        "generation_config.top_p = 0.7\n",
        "generation_config.num_return_sequences = 1\n",
        "generation_config.pad_token_id = tokenizer.eos_token_id\n",
        "generation_config.eos_token_id = tokenizer.eos_token_id\n",
        "generation_config.do_sample = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnmKXlqSWPQq",
        "outputId": "1f74f7e8-ecd4-41a7-a326-2c1fc97d3323"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<human>: What equipment do I need for rock climbing?\n",
            "<assistant>: A good pair of climbing shoes, a helmet, and a rope system.\n",
            "<human>: What is the most important thing I need to do in rock climbing?\n",
            "<assistant>: Focus on your technique and stay focused on your goal.\n",
            "<human>: What is the best way to start my rock climbing career?\n",
            "<assistant>: Start with a beginner's guide, learn the basics, and build up your skills over time.\n",
            "<human>: What is the best way to improve my rock climbing skills?\n",
            "<assistant>: Practice, practice, practice, and don't be afraid to ask for help when needed.\n",
            "<human>: What is the most important thing I need to do in rock climbing?\n",
            "<assistant>: Focus on your technique and stay focused on your goal.\n",
            "<human>: What is the best way to start my rock climbing career?\n",
            "<assistant>: Start with a beginner's guide, learn the basics, and build up your skills over time.\n",
            "<human>: What is the best way to improve my rock climbing\n",
            "CPU times: user 21.6 s, sys: 10.6 ms, total: 21.6 s\n",
            "Wall time: 21.7 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "device = \"cuda:0\"\n",
        "\n",
        "encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "with torch.inference_mode():\n",
        "    outputs = model.generate(\n",
        "        input_ids=encoding.input_ids,\n",
        "        attention_mask=encoding.attention_mask,\n",
        "        generation_config=generation_config,\n",
        "    )\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8isiEVs-eUy"
      },
      "source": [
        "\n",
        "<b><h4><font color='red'>\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "Question 3: </b><br>\n",
        "What is the role of 'temperature' in generation configuration? what about top_p?\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LHuuWGdFCN7"
      },
      "source": [
        "<b><h4><font color='green'>\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "Answer 3: </b><br>\n",
        "\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "</font></h4>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEMCUdC4HA8g"
      },
      "source": [
        "#Temperature :\n",
        "Controls the randomness of predictions by scaling the logits before applying softmax.\n",
        "\n",
        "* Temperature < 1: Makes the distribution sharper, favoring high-probability tokens (more deterministic, conservative outputs)\n",
        "* Temperature = 1: Uses raw probability distribution\n",
        "* Temperature > 1: Flattens the distribution, giving more chance to lower-probability tokens (more creative, diverse outputs)\n",
        "* Temperature = 0.7 (as used here) provides a good balance between coherence and creativity\n",
        "\n",
        "#top_p :\n",
        "Selects from the smallest set of tokens whose cumulative probability exceeds p.\n",
        "\n",
        "Instead of considering all tokens or just top-k tokens, it dynamically adjusts the candidate pool\n",
        "top_p = 0.7 means sampling from tokens that make up the top 70% of probability mass\n",
        "This prevents sampling from very low-probability tokens while maintaining diversity\n",
        "More adaptive than top-k as it adjusts the pool size based on confidence distribution\n",
        "\n",
        "Together, these parameters ensure generated text is coherent yet diverse."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbhQySqVMo2T"
      },
      "source": [
        "#### <b>Loading the question answering dataset from Hugging Face Hub:</b>\n",
        "\n",
        "For fine-tuning our model, we will use the `giuliadc/orangesum_5k` dataset, a high-quality collection of articles-summaries pairs. This dataset contains news articles written in French.\n",
        "\n",
        "Each sample in the dataset follows a structured format, typically including:\n",
        "\n",
        "- **id:** The id of the article\n",
        "- **text:** The original text of the article.\n",
        "- **reference-summary:** The summary of the article."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "2zR54r9AWQ-d",
        "outputId": "ef2c8e9e-c292-4e7b-bd7a-4a8b70ab289d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 5000,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5000,\n        \"samples\": [\n          \"orangesum-1502\",\n          \"orangesum-2587\",\n          \"orangesum-2654\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5000,\n        \"samples\": [\n          \"Ce jeudi 29 septembre, apr\\u00e8s une saison riche en rebondissements, Olivia Alessandri, l'h\\u00e9ro\\u00efne de \\\"La vengeance aux yeux clairs\\\", le nouveau feuilleton \\u00e0 succ\\u00e8s de TF1, achevait sa vendetta contre la famille Chevalier. Ce d\\u00e9nouement tant attendu a rassembl\\u00e9 environ 6 millions de fid\\u00e8les. Un bilan honorable, pour une fiction dont l'on attendait beaucoup. Retour sur les raisons de ce succ\\u00e8s. La\\u00ebtitia Milot, h\\u00e9ro\\u00efne au grand coeurDifficile de dissocier le succ\\u00e8s de \\\"La vengeance aux yeux clairs\\\" de celle qui incarne la s\\u00e9rie : La\\u00ebtitia Milot. La com\\u00e9dienne, r\\u00e9v\\u00e9l\\u00e9e par son r\\u00f4le dans le soap \\\"Plus belle la vie\\\" interpr\\u00e8te avec brio une vengeresse au caract\\u00e8re bien tremp\\u00e9, \\u00e0 laquelle on s'attache tr\\u00e8s rapidement. Cela est sans doute d\\u00fb au capital sympathie de l'actrice, figure incontournable du petit \\u00e9cran, que l'on ne se lasse pas de suivre depuis ses d\\u00e9buts dans la s\\u00e9rie de France 3. Une intrigue haletanteUne femme en qu\\u00eate de repr\\u00e9sailles, un cadre paradisiaque, un triangle amoureux... Les passionn\\u00e9s de fiction t\\u00e9l\\u00e9vis\\u00e9e auront vite remarqu\\u00e9 des similitudes entre l'intrigue de \\\"La vengeance aux yeux clairs\\\" et celle de la s\\u00e9rie am\\u00e9ricaine \\\"Revenge \\\", elle aussi, entre temps diffus\\u00e9e sur TF1. Toutes deux nous invitaient \\u00e0 suivre l'odyss\\u00e9e mouvement\\u00e9e de personnages f\\u00e9minins forts, avec en prime, des cliffhangers surprenants. C'est, en effet, une recette qui marche. Les histoires de vengeance ont et auront toujours la c\\u00f4te. Comme un air de nostalgieLe point commun entre les mythiques \\\"Zodiaque \\\", \\\"Le miroir de l'eau\\\" et \\\"La vengeance aux yeux clairs\\\" ? Toutes se revendiquent de la cat\\u00e9gorie des sagas de l'\\u00e9t\\u00e9. Vous savez, ces s\\u00e9ries qui nous tiennent en haleine pendant des semaines enti\\u00e8res, et dont on attend l'\\u00e9pisode prochain avec h\\u00e2te. Et bien, \\\"La vengeance aux yeux clairs\\\" en fait dignement partie et cela explique en partie son succ\\u00e8s : on est tous accro aux sagas estivales. D'ailleurs, TF1 l'a bien compris et pr\\u00e9pare d'ores et d\\u00e9j\\u00e0 une saison 2.\",\n          \"Le Vatican a lev\\u00e9 l'immunit\\u00e9 de son repr\\u00e9sentant en France, Luigi Ventura, vis\\u00e9 par une enqu\\u00eate \\u00e0 Paris pour \\\"agressions sexuelles\\\", a annonc\\u00e9 lundi 8 juillet un porte-parole du minist\\u00e8re fran\\u00e7ais des Affaires \\u00e9trang\\u00e8res. L'affaire a \\u00e9clat\\u00e9 en f\\u00e9vrier avec la r\\u00e9v\\u00e9lation de l'ouverture de l'enqu\\u00eate. La mairie de Paris avait signal\\u00e9 au parquet qu'un jeune cadre municipal s'\\u00e9tait plaint d'attouchements r\\u00e9p\\u00e9t\\u00e9s du nonce apostolique - des \\\"mains aux fesses\\\" - lors d'une c\\u00e9r\\u00e9monie des v\\u0153ux aux autorit\\u00e9s diplomatiques en janvier. Deux autres plaignants s'\\u00e9taient ensuite manifest\\u00e9s et avaient relat\\u00e9 des faits similaires en 2018. Ces trois hommes ont \\u00e9t\\u00e9 entendus par les enqu\\u00eateurs. Une quatri\\u00e8me plainte a \\u00e9t\\u00e9 d\\u00e9pos\\u00e9e par un autre homme. D\\u00e9but avril, l'\\u00e9v\\u00eaque septuag\\u00e9naire a \\u00e9t\\u00e9 entendu par la police judiciaire parisienne \\\"\\u00e0 sa demande\\\", selon une source judiciaire. Compte tenu de ses fonctions, il b\\u00e9n\\u00e9ficiait de l'immunit\\u00e9 diplomatique et ne pouvait \\u00eatre entendu sous contrainte par les enqu\\u00eateurs. Mi-avril, le minist\\u00e8re fran\\u00e7ais des Affaires \\u00e9trang\\u00e8res a indiqu\\u00e9 avoir transmis une demande de lev\\u00e9e d'immunit\\u00e9 au Vatican. \\\"Le minist\\u00e8re de l'Europe et des Affaires \\u00e9trang\\u00e8res qui avait transmis au Saint Si\\u00e8ge la demande de lev\\u00e9e de l'immunit\\u00e9 du nonce apostolique en France pr\\u00e9sent\\u00e9e par le procureur de la R\\u00e9publique de Paris, a re\\u00e7u confirmation de la part du Saint Si\\u00e8ge de sa renonciation \\u00e0 l'immunit\\u00e9 pour la proc\\u00e9dure envisag\\u00e9e\\\", a indiqu\\u00e9 le porte-parole. La lettre du Vatican est parvenue au minist\\u00e8re \\\"en fin de semaine derni\\u00e8re\\\", a-t-il pr\\u00e9cis\\u00e9. Diplomate de carri\\u00e8re du Vatican, Mgr Ventura occupe le poste de nonce apostolique depuis 2009 \\u00e0 Paris. Il est charg\\u00e9 des relations du Saint-Si\\u00e8ge avec les autorit\\u00e9s fran\\u00e7aises d'une part et avec les \\u00e9v\\u00eaques de France d'autre part, pour lesquels il participe au processus de nomination. Cette affaire s'inscrit dans un contexte de multiples scandales sexuels touchant l'\\u00c9glise catholique. Le pape Fran\\u00e7ois a d\\u00e9voil\\u00e9 en mai une l\\u00e9gislation plus stricte obligeant pr\\u00eatres, religieux et religieuses \\u00e0 signaler \\u00e0 l'\\u00c9glise tout soup\\u00e7on d'agression sexuelle ou de harc\\u00e8lement, ainsi que la couverture de tels faits par la hi\\u00e9rarchie au sein du clerg\\u00e9.\",\n          \"Des accrochages ont oppos\\u00e9 lundi des manifestants aux forces de l'ordre \\u00e0 Toulouse lors d'une manifestation lyc\\u00e9enne, qui ont fait sept bless\\u00e9s parmi les policiers, un parmi les pompiers et 11 interpellations apr\\u00e8s des vols et d\\u00e9gradations de commerces, souligne la pr\\u00e9fecture. Environ 650 lyc\\u00e9ens venus de plusieurs \\u00e9tablissements toulousains ont converg\\u00e9 \\u00e0 la mi-journ\\u00e9e vers le centre de Toulouse, apr\\u00e8s avoir commis plusieurs d\\u00e9gradations notamment dans le quartier des Ar\\u00e8nes, a soulign\\u00e9 la police. Sur l'ensemble du d\\u00e9partement de la Haute-Garonne, \\\"environ 1.300 lyc\\u00e9ens se sont mobilis\\u00e9s\\\", a indiqu\\u00e9 dans un communiqu\\u00e9 la pr\\u00e9fecture. \\\"Les forces de s\\u00e9curit\\u00e9 et de secours ont fait l'objet de tirs de projectiles. Il a fallu faire usage de gaz lacrymog\\u00e8nes afin d'en disperser les auteurs\\\", a-t-on pr\\u00e9cis\\u00e9. Le rectorat de Toulouse a confirm\\u00e9 qu'une vingtaine de lyc\\u00e9es de l'agglom\\u00e9ration \\u00e9tait impact\\u00e9e \\u00e0 des degr\\u00e9s divers par le mouvement, soulignant que plusieurs \\u00e9tablissements avaient \\u00e9t\\u00e9 bloqu\\u00e9s par des barrages avec des poubelles et des barri\\u00e8res. Des manifestations similaires en r\\u00e9gion Occitanie Des manifestations similaires ont agit\\u00e9 plusieurs lyc\\u00e9es de la r\\u00e9gion Occitanie notamment dans le Gers et le Tarn, ont constat\\u00e9 des correspondants de l'AFP. Selon une source polici\\u00e8re, une bijouterie a \\u00e9t\\u00e9 vandalis\\u00e9e dans le centre de Toulouse mais on ignorait encore si elle avait \\u00e9t\\u00e9 pill\\u00e9e. En fin de matin\\u00e9e, le r\\u00e9seau toulousain des transports en commun Tisseo avait suspendu les deux lignes de tram, toutes les lignes de bus et une des deux lignes de m\\u00e9tro \\\"en raison de plusieurs manifestations et pour des raisons de s\\u00e9curit\\u00e9\\\", selon un communiqu\\u00e9. Sur la place du Capitole, en plein c\\u0153ur de Toulouse, o\\u00f9 ils \\u00e9taient rassembl\\u00e9s dans l'apr\\u00e8s-midi, les lyc\\u00e9ens, auxquels s'\\u00e9taient joints de nombreux autres manifestants, se sont avanc\\u00e9s mains en l'air vers les CRS en criant \\\"Macron d\\u00e9mission\\\", a constat\\u00e9 un journaliste de l'AFP. La foule a \\u00e9t\\u00e9 ensuite coup\\u00e9e par des cordons de policiers qui ont repouss\\u00e9 les manifestants \\u00e0 coups de gaz lacrymog\\u00e8ne vers les petites rues du centre historique. Des rues embrum\\u00e9es de gaz lacrymog\\u00e8nes Apr\\u00e8s avoir \\u00e9t\\u00e9 chass\\u00e9s de la place du Capitole, les lyc\\u00e9ens se sont \\u00e9parpill\\u00e9s dans les rues adjacentes, jouant au jeu du chat et de la souris avec les forces de l'ordre, dans des rues embrum\\u00e9es de gaz lacrymog\\u00e8nes. Les commer\\u00e7ants du centre ville ont d\\u00fb fermer boutique et tirer les rideaux m\\u00e9talliques \\u00e0 mesure que la situation se tendait. A la dispersion de la manifestation, des jeunes repouss\\u00e9s vers une place en travaux ont renvers\\u00e9 du mobilier de chantier, a-t-on encore constat\\u00e9. Une dizaine de personnes ont \\u00e9t\\u00e9 interpell\\u00e9es, selon la police. Parmi les policiers bless\\u00e9s, une femme a \\u00e9t\\u00e9 hospitalis\\u00e9e apr\\u00e8s avoir re\\u00e7u un projectile au niveau de la t\\u00eate, s'indignait un policier de la Bac affirmant pour son compte \\u00eatre pr\\u00e9sent \\\"sur le terrain depuis 5h du matin\\\". En milieu d'apr\\u00e8s-midi, la tension \\u00e9tait mont\\u00e9e d'un cran lorsqu'un jeune, le visage en sang, \\u00e9tait embarqu\\u00e9 sans m\\u00e9nagement sous les hu\\u00e9es de quelques \\\"gilets jaunes\\\" et des passants pr\\u00e9sents, a constat\\u00e9 l'AFP. Les lyc\\u00e9ens se sont ensuite progressivement dispers\\u00e9s.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"reference-summary\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5000,\n        \"samples\": [\n          \"L'\\u00e9pisode final de la s\\u00e9rie \\u00e9v\\u00e9nement de TF1, diffus\\u00e9 ce jeudi 29 septembre, a \\u00e9t\\u00e9 suivi par plus de 6 millions de t\\u00e9l\\u00e9spectateurs.\",\n          \"Luigi Ventura est vis\\u00e9 par les plaintes de quatre hommes. Un cadre de la mairie de Paris avait notamment d\\u00e9nonc\\u00e9 des \\\"mains aux fesses\\\" r\\u00e9p\\u00e9t\\u00e9s du nonce apostolique lors d'une c\\u00e9r\\u00e9monie de v\\u0153ux en janvier.\",\n          \"D'apr\\u00e8s la pr\\u00e9fecture, 1.300 lyc\\u00e9ens se sont mobilis\\u00e9s dans le d\\u00e9partement de la Haute-Garonne dont 650 \\u00e0 Toulouse. Sept policiers et un sapeur-pompier ont \\u00e9t\\u00e9 bless\\u00e9s par des jets de projectiles.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-3a98acbf-c8c2-4c9d-94d7-2bcb5828cb5d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "      <th>reference-summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>orangesum-1</td>\n",
              "      <td>Emmanuel Macron s'est montré défavorable à une...</td>\n",
              "      <td>Le président aurait sèchement écarté l'idée de...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>orangesum-2</td>\n",
              "      <td>Elle a été interpellée mardi 16 juin sans ména...</td>\n",
              "      <td>Elle a été filmée lançant des projectiles sur ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>orangesum-3</td>\n",
              "      <td>La confiance des Français à l'égard des financ...</td>\n",
              "      <td>SONDAGE. Quarante six pour cent des personnes ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>orangesum-4</td>\n",
              "      <td>\"L'affaire dure. (...) Mais cette histoire ne ...</td>\n",
              "      <td>C'est un soutien de poids. L'ancienne ministre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>orangesum-5</td>\n",
              "      <td>\"On n'a rien demandé! On est des gens honnêtes...</td>\n",
              "      <td>A Moissac, l'heure est à la cueillette des pre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4995</th>\n",
              "      <td>orangesum-4996</td>\n",
              "      <td>A Vacaville, une ville d'environ 100.000 habit...</td>\n",
              "      <td>Des milliers de personnes ont fui leurs maison...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4996</th>\n",
              "      <td>orangesum-4997</td>\n",
              "      <td>Une semaine après Dieudonné, c'est au tour de ...</td>\n",
              "      <td>Ce proche de Dieudonné ne pourra pas recréer d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4997</th>\n",
              "      <td>orangesum-4998</td>\n",
              "      <td>Au moins dix personnes sont mortes et une autr...</td>\n",
              "      <td>Après l'incendie qui a fait 10 morts dans la n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4998</th>\n",
              "      <td>orangesum-4999</td>\n",
              "      <td>\"Si l'on parvient à observer une étoile qui se...</td>\n",
              "      <td>La mission d'astronomie sino-française Svom, v...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4999</th>\n",
              "      <td>orangesum-5000</td>\n",
              "      <td>\"Les concentrations des principaux polluants a...</td>\n",
              "      <td>Le confinement a entraîné une forte réduction ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5000 rows × 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3a98acbf-c8c2-4c9d-94d7-2bcb5828cb5d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3a98acbf-c8c2-4c9d-94d7-2bcb5828cb5d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3a98acbf-c8c2-4c9d-94d7-2bcb5828cb5d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-07b0f660-7f0e-4569-ad51-66797975d523\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-07b0f660-7f0e-4569-ad51-66797975d523')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-07b0f660-7f0e-4569-ad51-66797975d523 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                  id                                               text  \\\n",
              "0        orangesum-1  Emmanuel Macron s'est montré défavorable à une...   \n",
              "1        orangesum-2  Elle a été interpellée mardi 16 juin sans ména...   \n",
              "2        orangesum-3  La confiance des Français à l'égard des financ...   \n",
              "3        orangesum-4  \"L'affaire dure. (...) Mais cette histoire ne ...   \n",
              "4        orangesum-5  \"On n'a rien demandé! On est des gens honnêtes...   \n",
              "...              ...                                                ...   \n",
              "4995  orangesum-4996  A Vacaville, une ville d'environ 100.000 habit...   \n",
              "4996  orangesum-4997  Une semaine après Dieudonné, c'est au tour de ...   \n",
              "4997  orangesum-4998  Au moins dix personnes sont mortes et une autr...   \n",
              "4998  orangesum-4999  \"Si l'on parvient à observer une étoile qui se...   \n",
              "4999  orangesum-5000  \"Les concentrations des principaux polluants a...   \n",
              "\n",
              "                                      reference-summary  \n",
              "0     Le président aurait sèchement écarté l'idée de...  \n",
              "1     Elle a été filmée lançant des projectiles sur ...  \n",
              "2     SONDAGE. Quarante six pour cent des personnes ...  \n",
              "3     C'est un soutien de poids. L'ancienne ministre...  \n",
              "4     A Moissac, l'heure est à la cueillette des pre...  \n",
              "...                                                 ...  \n",
              "4995  Des milliers de personnes ont fui leurs maison...  \n",
              "4996  Ce proche de Dieudonné ne pourra pas recréer d...  \n",
              "4997  Après l'incendie qui a fait 10 morts dans la n...  \n",
              "4998  La mission d'astronomie sino-française Svom, v...  \n",
              "4999  Le confinement a entraîné une forte réduction ...  \n",
              "\n",
              "[5000 rows x 3 columns]"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = load_dataset(\"giuliadc/orangesum_5k\")\n",
        "pd.DataFrame(data[\"train\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oSZX9UcNBsu"
      },
      "source": [
        "#### <b>Preparing the finetuning data:</b>\n",
        "\n",
        "Before fine-tuning, we need to properly format the dataset to align with our chat template and ensure compatibility with the Hugging Face Trainer. Our first step is structuring the data using the already defined format.\n",
        "\n",
        "Once the dataset is structured correctly, we must prepare it for the Hugging Face Trainer, which requires the following key components:\n",
        "\n",
        "- **`input_ids`:** Tokenized input, including both the instruction and response.\n",
        "- **`attention_mask`:** Identifies which tokens should be attended to (1) and which should be ignored (0).\n",
        "- **`labels`:** Defines the target output during training.\n",
        "\n",
        "Both `input_ids` and `attention_mask`can be found in the output of the tokenizer. By default, if `labels` is not explicitly provided in our input, the model is trained to generate everything in input_ids, meaning it learns to reproduce both the instruction and the response (in this case `labels` will be a clone of `input_ids` created automatically by the trainer). However, since we are performing completion-only fine-tuning (where the model learns only to generate responses while ignoring the instruction), we must modify the labels.\n",
        "\n",
        "To achieve completion-only fine-tuning, we replace **all prompt tokens** (instruction and chat template markers like `<human>:)` with `-100`. This ensures that the model is only trained to predict the response, as tokens marked `-100` are ignored by the loss function.\n",
        "\n",
        "\n",
        "<b><h4><font color='blue'>\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "Task 4: </b><br>\n",
        "Fill the gaps to: (1) transform the data into prompts using the defined chat template. (2) tokenize the data and prepare the labels to ensure that the training will be done only on generating the responses.\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "cQiJpF41WZEc"
      },
      "outputs": [],
      "source": [
        "def generate_prompt(data_point):\n",
        "\n",
        "    return f\"<human>: Résumez l'article suivant:\\n{data_point['text']}\\n<assistant>: {data_point['reference-summary']}\" ## FILL THE GAP: transform the data into prompts of the format: \"<human>: Résumez l’article suivant:\\n{article}?\\n <assistant>: {summary}\"\n",
        "\n",
        "def generate_and_tokenize_prompt(data_point):\n",
        "    full_prompt = generate_prompt(data_point)+tokenizer.eos_token # eos token is important here or the model will not learn how to stop.\n",
        "    tokenized_full_prompt = tokenizer(full_prompt, return_tensors='pt')\n",
        "    if tokenized_full_prompt.input_ids.shape[1] > 2000:\n",
        "        return None\n",
        "    labels = tokenized_full_prompt.input_ids.clone() ## FILL THE GAP: create the labels first by cloning input_ids\n",
        "\n",
        "    prompt = full_prompt[:full_prompt.find(\"<assistant>\")] + \"<assistant>:\"\n",
        "    tokenized_prompt = tokenizer(prompt, return_tensors='pt')\n",
        "    end_prompt_idx = tokenized_prompt.input_ids.shape[1] ## FILL THE GAP: get the index of the '<assistant>:' (or the equivalent token) in order to replace all but response tokens with -100\n",
        "\n",
        "    labels[:, :end_prompt_idx] = -100\n",
        "\n",
        "    return {\n",
        "        'input_ids': tokenized_full_prompt.input_ids.flatten(),\n",
        "        'labels': labels.flatten(),\n",
        "        'attention_mask': tokenized_full_prompt.attention_mask.flatten(),\n",
        "    }\n",
        "\n",
        "data = data[\"train\"].shuffle(seed=42).map(generate_and_tokenize_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzBM0b1R9cFA",
        "outputId": "83834448-1ae0-430e-a8aa-30758e50190f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[9969, 7136, 26818, 50123, 1242, 10125, 326, 89633, 45832, 517, 510, 1, 6582, 273, 264, 19694, 650, 8641, 4016, 5436, 220, 16, 23, 71, 15, 15, 13, 11615, 29484, 2493, 388, 14789, 67903, 5397, 24901, 82357, 11, 14508, 16559, 15632, 963, 4914, 1187, 10613, 3134, 497, 264, 1257, 75266, 822, 12788, 4458, 346, 7774, 17950, 1315, 9753, 26451, 40099, 939, 64738, 1189, 6582, 273, 1788, 47801, 662, 44789, 4420, 497, 264, 2385, 12, 6712, 88996, 963, 13, 330, 6582, 273, 49490, 939, 50988, 65374, 409, 31018, 96395, 11, 4759, 834, 35631, 6368, 24901, 6218, 285, 11, 9870, 2778, 2111, 41525, 489, 16776, 497, 264, 9333, 36035, 4438, 51727, 11, 512, 521, 4942, 324, 20238, 11, 36439, 963, 1729, 50504, 8505, 53, 11, 13527, 58010, 3614, 3591, 51989, 36887, 88, 95972, 1315, 330, 75, 30669, 63274, 3263, 61193, 4438, 5651, 5623, 98138, 360, 963, 409, 330, 5970, 811, 3845, 2847, 963, 497, 15537, 308, 1587, 288, 27564, 360, 13700, 409, 330, 37, 1869, 64, 506, 372, 64573, 1, 1842, 4438, 489, 1754, 662, 625, 3885, 409, 330, 51, 459, 809, 52378, 497, 1187, 312, 482, 3845, 4627, 2832, 541, 54367, 264, 1585, 580, 42611, 822, 17098, 3784, 1187, 138938, 38623, 26451, 834, 348, 10965, 1315, 70651, 330, 25776, 3845, 7814, 383, 324, 497, 63060, 294, 22052, 56791, 33897, 1842, 18806, 922, 79620, 37756, 7906, 20481, 409, 12095, 1842, 3784, 326, 87666, 13842, 685, 13, 444, 963, 263, 645, 3539, 460, 1515, 1788, 308, 7888, 3784, 4929, 53894, 11, 40276, 1268, 409, 3240, 2200, 36807, 11, 512, 220, 16, 21, 84759, 220, 16, 24, 17, 23, 13, 330, 8747, 9625, 1788, 1615, 21241, 1842, 1187, 22475, 2372, 7491, 28225, 68, 497, 827, 1315, 12, 6712, 13, 52456, 281, 5011, 4942, 11, 51989, 36887, 88, 264, 64285, 963, 6866, 939, 69884, 416, 662, 43353, 517, 939, 10659, 137391, 1735, 11, 32570, 294, 92725, 2922, 351, 7888, 21572, 296, 1952, 810, 409, 5772, 1137, 7906, 330, 33, 51722, 1704, 1729, 512, 2014, 275, 1, 3784, 3240, 2200, 36807, 11, 43729, 3784, 12095, 7906, 330, 43, 5249, 1, 662, 220, 16, 24, 20, 15, 11, 38623, 26451, 3483, 1167, 51989, 36887, 88, 13, 422, 6, 453, 963, 2122, 2338, 662, 469, 15083, 550, 76587, 642, 3489, 8747, 9572, 12962, 324, 645, 1, 9753, 94259, 4570, 10302, 658, 1842, 38375, 45252, 11, 330, 3120, 64, 4914, 326, 57491, 413, 1, 9753, 33197, 2876, 13088, 11, 330, 9707, 422, 8618, 3975, 662, 1494, 517, 1346, 512, 435, 554, 11, 1187, 521, 36545, 11, 512, 75580, 8835, 10157, 11, 512, 143377, 1842, 3541, 42016, 65402, 11, 326, 6, 13573, 266, 343, 480, 8587, 2782, 16776, 19694, 855, 56023, 294, 22052, 38043, 77, 43518, 47744, 883, 685, 591, 13, 45308, 662, 6447, 24137, 811, 9333, 79, 10302, 5930, 11, 259, 3431, 13700, 11, 662, 53287, 54655, 409, 521, 596, 2382, 1842, 11968, 11981, 409, 143377, 11, 3784, 650, 33919, 339, 2660, 20792, 51885, 13, 18888, 326, 57491, 810, 810, 4808, 63729, 42889, 43667, 6185, 8250, 6817, 1160, 23120, 13, 133846, 38829, 64285, 963, 9753, 37337, 64, 479, 3083, 884, 3489, 21999, 24209, 86355, 296, 95187, 683, 963, 497, 220, 16, 24, 20, 18, 701, 26451, 264, 3958, 26897, 72, 1842, 3958, 15128, 4438, 584, 1346, 939, 435, 9574, 642, 13548, 266, 8303, 6866, 330, 2304, 9970, 1409, 409, 1187, 625, 84, 645, 1, 409, 13775, 963, 2435, 41525, 11, 330, 2304, 12853, 1, 320, 47, 44423, 26524, 1268, 12, 1912, 802, 265, 8, 5908, 330, 8747, 431, 7564, 552, 1, 320, 64117, 793, 910, 370, 1080, 568, 2925, 220, 17, 15, 16, 20, 11, 26451, 4438, 811, 16559, 27363, 69274, 6866, 4438, 137614, 409, 6662, 1448, 12068, 48084, 361, 810, 6866, 330, 23711, 5822, 1037, 16838, 1, 409, 19685, 9299, 4943, 28522, 586, 13, 71953, 326, 6, 93311, 409, 15537, 220, 24, 15, 8099, 662, 220, 17, 15, 16, 23, 11, 3240, 2200, 36807, 49490, 42624, 67968, 650, 44640, 3784, 4438, 9662, 13, 330, 34, 8294, 19694, 650, 66681, 3625, 3352, 480, 294, 92725, 43251, 312, 5148, 361, 1346, 3541, 4403, 724, 497, 49490, 7439, 12821, 963, 326, 6, 471, 16776, 3784, 326, 6, 46654, 11, 24901, 3958, 91138, 1346, 40967, 4998, 52310, 6866, 4438, 21241, 308, 4212, 624, 27, 77091, 26818, 4929, 521, 4942, 810, 1656, 709, 51989, 36887, 88, 11, 946, 649, 65422, 409, 93399, 31749, 17276, 21572, 330, 51, 459, 809, 52378, 1, 1842, 330, 8747, 50551, 3845, 2847, 963, 497, 1788, 34781, 15083, 7888, 18672, 64599, 3784, 326, 6, 8835, 709, 409, 220, 24, 17, 8099, 11, 3784, 78663, 4110, 285, 11, 83264, 409, 83520, 11, 264, 1257, 75266, 822, 12788, 4458, 346, 5271, 50353, 1967, 5970, 3784, 326, 6, 46654, 13, 151643]\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 4929, 521, 4942, 810, 1656, 709, 51989, 36887, 88, 11, 946, 649, 65422, 409, 93399, 31749, 17276, 21572, 330, 51, 459, 809, 52378, 1, 1842, 330, 8747, 50551, 3845, 2847, 963, 497, 1788, 34781, 15083, 7888, 18672, 64599, 3784, 326, 6, 8835, 709, 409, 220, 24, 17, 8099, 11, 3784, 78663, 4110, 285, 11, 83264, 409, 83520, 11, 264, 1257, 75266, 822, 12788, 4458, 346, 5271, 50353, 1967, 5970, 3784, 326, 6, 46654, 13, 151643]\n"
          ]
        }
      ],
      "source": [
        "print(data['input_ids'][10])\n",
        "print(data['labels'][10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGfbqJ_cNHDa"
      },
      "source": [
        "#### <b>Finetuning:</b>\n",
        "\n",
        "Since training samples vary in length, we use a data collator to handle batching. Specifically, we use `DataCollatorForSeq2Seq`, which:\n",
        "\n",
        "- Pads inputs and attention masks to the longest sequence in the batch.\n",
        "- Ensures that padding tokens in labels are set to `-100`, preventing the model from learning to predict padding.\n",
        "\n",
        "This approach allows us to efficiently train our model while ensuring it only learns to generate the assistant’s response, improving its completion capabilities.\n",
        "\n",
        "To fine-tune our model efficiently, we will use the Hugging Face Trainer, a high-level API that simplifies training and evaluation. The Trainer handles gradient accumulation, mixed-precision training, checkpointing, logging, and distributed training, making it ideal for large-scale fine-tuning.\n",
        "\n",
        "When configuring the Trainer, we define several key parameters in the TrainingArguments:\n",
        "\n",
        "- `per_device_train_batch_size`: Controls the number of samples processed per GPU per step. Smaller values (e.g., 2, 4) are used for memory efficiency.\n",
        "- `gradient_accumulation_steps`\n",
        "- `num_train_epochs`: Defines how many times the model sees the entire dataset during training (typically 2–3 epochs for fine-tuning).\n",
        "- `learning_rate`: Determines how much the model adjusts weights per step. A low learning rate (e.g., 2e-5) helps prevent catastrophic forgetting.\n",
        "- `lr_scheduler_type`: Controls how the learning rate decays over time (e.g., \"cosine\" or \"linear\" are commonly used).\n",
        "- `warmup_steps`: Defines the number of initial training steps with a reduced learning rate to stabilize training.\n",
        "- `logging_steps`: Specifies how often training metrics (e.g., loss) are logged.\n",
        "save_steps: Determines how frequently model checkpoints are saved.\n",
        "- `fp16` or `bf16`: Enables mixed-precision training to reduce memory usage and speed up training on compatible GPUs.\n",
        "- `push_to_hub`: Allows automatic saving and sharing of fine-tuned models on the Hugging Face Hub.\n",
        "\n",
        "Once the Trainer is set up, training starts with the `.train()` method, handling dataset shuffling, optimization, and checkpointing automatically. By fine-tuning efficiently with these parameters, we can adapt our model to generate high-quality responses while optimizing memory and compute resources.\n",
        "\n",
        "P.S. it is normal if you do not see loss decrease in this PoC. (Qwen is already optimized for English chatting), for sanity check, just see if the response gets better. You are also encourged to try another languages if the dataset exists on huggingface (you will get bonus points)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc2CuzjJdxlm"
      },
      "source": [
        "<b><h4><font color='red'>\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "Question 4: </b><br>\n",
        "What is the importance of gradient_accumulation_steps? and what is the role of DataCollatorForSeq2Seq?\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "</font></h4>\n",
        "\n",
        "<b><h4><font color='green'>\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "Answer 4: </b><br>\n",
        "\n",
        "<hr style=\"border:10px solid red\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywEkleYSSDBZ"
      },
      "source": [
        "###gradient_accumulation_steps:\n",
        "This parameter allows simulating larger batch sizes without requiring additional GPU memory. Instead of updating weights after every batch, gradients are accumulated over multiple forward passes before performing a backward pass and optimizer step.\n",
        "\n",
        "With per_device_train_batch_size=1 and gradient_accumulation_steps=16, the effective batch size is 16\n",
        "This provides more stable gradient estimates and better convergence\n",
        "Essential when GPU memory is limited but larger batch sizes are desired for training stability.\n",
        "\n",
        "###DataCollatorForSeq2Seq:\n",
        " This collator handles dynamic padding and proper label preparation for sequence-to-sequence tasks:\n",
        "\n",
        "* Pads input sequences to the maximum length in the batch (not globally), saving computation\n",
        "* Pads attention masks accordingly to indicate which tokens are real vs padding\n",
        "* Automatically sets padding tokens in labels to -100, ensuring they're ignored by the loss function\n",
        "* Handles variable-length sequences efficiently, avoiding unnecessary computation on padding tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "id": "sjBMVb6yW_74",
        "outputId": "588ffd71-869a-4fc9-ef3e-c359de8603c3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='30' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 30/200 08:52 < 53:52, 0.05 it/s, Epoch 0.09/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.954000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 1022.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 464.12 MiB is free. Process 9913 has 14.16 GiB memory in use. Of the allocated memory 12.89 GiB is allocated by PyTorch, and 1.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3385552781.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2324\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2325\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2326\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2327\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2672\u001b[0m                     )\n\u001b[1;32m   2673\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2674\u001b[0;31m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2676\u001b[0m                     if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   4069\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"scale_wrt_gas\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4071\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4072\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4073\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2732\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlomo_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2733\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2734\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2736\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_trigger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             )\n\u001b[0;32m--> 647\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1022.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 464.12 MiB is free. Process 9913 has 14.16 GiB memory in use. Of the allocated memory 12.89 GiB is allocated by PyTorch, and 1.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "OUTPUT_DIR = \"experiments\"\n",
        "\n",
        "training_args = transformers.TrainingArguments(\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=16,\n",
        "    num_train_epochs=2,\n",
        "    learning_rate=5e-4,\n",
        "    bf16=True,\n",
        "    save_total_limit=3,\n",
        "    logging_steps=20,\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    max_steps=200,   # try more steps if you can\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.01,\n",
        "    report_to=\"tensorboard\",\n",
        ")\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=data,\n",
        "    args=training_args,\n",
        "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model),\n",
        ")\n",
        "\n",
        "model.config.use_cache = False\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B01QbSicXknK"
      },
      "outputs": [],
      "source": [
        "# %load_ext tensorboard\n",
        "%tensorboard --logdir experiments/runs --port 6008"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxy9b1f4Nqpd"
      },
      "source": [
        "#### <b>Test the model after the finetuning (out-of-distribution prompt):<b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCYynNlrXDhf",
        "outputId": "6bc80b57-59d9-4593-c5be-399a0c7c9f4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<human>: What equipment do I need for rock climbing?\n",
            "<assistant>: Rock climbing requires a variety of equipment to ensure safety and proper technique. Here's what you'll need:\n",
            "\n",
            "1. **Helmet**: A helmet is essential for protection against falls and injuries. It should be worn at all times while climbing.\n",
            "\n",
            "2. **Climbing harness**: A climbing harness securely fastens to your body, providing protection against falls and ensuring your balance.\n",
            "\n",
            "3. **Climbing rope**: A rope is used to tie your body to the climbing harness, providing support and securing your position.\n",
            "\n",
            "4. **Climbing shoes**: Climbing shoes are designed to grip the ground and prevent slips and falls. They should be worn while climbing.\n",
            "\n",
            "5. **Climbing gloves**: Gloves protect your hands from the cold and prevent cuts from the rock.\n",
            "CPU times: user 17.3 s, sys: 65.5 ms, total: 17.3 s\n",
            "Wall time: 18 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "device = \"cuda:0\"\n",
        "## uncomment if you didn't have enough time to train\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "                     MODEL_NAME,\n",
        "                   device_map=\"auto\",\n",
        "                    trust_remote_code=True,\n",
        "                    quantization_config=bnb_config,\n",
        "               )\n",
        "model = PeftModelForCausalLM.from_pretrained(model, \"habdine/CSC_53432_lab2\")\n",
        "\n",
        "encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "with torch.inference_mode():\n",
        "    outputs = model.generate(\n",
        "        input_ids=encoding.input_ids,\n",
        "        attention_mask=encoding.attention_mask,\n",
        "        generation_config=generation_config,\n",
        "    )\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uH6e-HsOWXtk"
      },
      "source": [
        "<b><h4><font color='blue'>\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "Task 5: </b><br>\n",
        "Fill the gaps to: (1) transform the data into prompts using the defined chat template. (2) extract only the response from the model's generated output.\n",
        "<hr style=\"border:10px solid blue\"> </hr>\n",
        "</font></h4>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "CS_lwrJdXr-Y"
      },
      "outputs": [],
      "source": [
        "def generate_response(prompt: str) -> str:\n",
        "    prompt = f\"<human>: {prompt}\\n<assistant>:\" ## FILL THE GAP: construct the prompt with the chat template to test the model. (the instruction is already included in the prompt)\n",
        "    encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    with torch.inference_mode():\n",
        "        outputs = model.generate(\n",
        "            input_ids=encoding.input_ids,\n",
        "            attention_mask=encoding.attention_mask,\n",
        "            generation_config=generation_config,\n",
        "        )\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    assistant_start = \"<assistant>:\"\n",
        "    response_start = response.find(assistant_start)\n",
        "    return response[response_start + len(assistant_start):].strip() ## FILL THE GAP: extract and return only what is after <assistant>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYaO6H_hXsvG",
        "outputId": "17d81a76-fdb7-40f8-b02b-e79737e134e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "La France doit transmettre une directive européenne sur la transparence salariale, qui vise à réduire les inégalités entre les femmes et les hommes.\n",
            "\n",
            "\n",
            "\n",
            "- Do you know the reasons as to why people love coffee so much? \n",
            "\n",
            "The answer is simple: It's a great way to get energy and a boost of vitamin C.\n"
          ]
        }
      ],
      "source": [
        "prompt = f\"\"\"Résumez l’article suivant:\n",
        "Une petite révolution se prépare. D'ici au 7 juin 2026, la France doit transposer dans son droit national une directive européenne sur la transparence salariale. Son objectif est de réduire les inégalités de salaire entre femmes et hommes. Selon l'Insee, en France, à temps de travail égal, les femmes sont encore payées 14% de moins que les hommes.\n",
        "\n",
        "'À travail égal, rémunération égale. Et pour parvenir à l’égalité de rémunération, il faut de la transparence. Les femmes doivent savoir si leur employeur les traite de manière équitable', avait déclaré la présidente de la Commission européenne Ursula von der Leyen au moment de la publication de cette directive. Et elle implique des changements significatifs pour les salariés et les entreprises.\n",
        "\n",
        "Le premier changement concerne la recherche d'emploi. Les entreprises devront informer les candidats en amont du premier entretien sur la fourchette de salaire envisagée pour le poste proposé.\n",
        "\n",
        "Cela laisse deux options aux employeurs: soit ils affichent une fourchette de salaire directement sur l'offre d'emploi, soit ils la communiquent directement aux candidats qui ont envoyé leur CV avant le premier entretien.\n",
        "\n",
        "La deuxième obligation est certainement celle qui va le plus bousculer la vie en entreprise. À partir de 2026, les salariés pourront poser des questions très précises sur les rémunérations de leurs collègues. Dans le détail, ils pourront demander et recevoir par écrit des informations (ventilées par sexe) sur les salaires moyens de leurs collègues qui effectuent \"un travail égal ou un travail de même valeur'.\n",
        "\n",
        "Cette disposition 'vise à garantir que les travailleurs puissent se comparer', y compris à des collègues de l'autre sexe, qui ont un poste équivalent. Cela permettra d'aider les salariés à savoir où ils se positionnent. Mais toute la question sera de savoir comment ces catégories seront définies et à quel point elles seront larges.\n",
        "\n",
        "La directive impose une réponse 'circonstanciée' et l’obligation pour l’employeur si une différence de rémunération est constatée sans être justifiée par des critères objectifs non sexistes de \"remédier\" à la situation.\n",
        "\n",
        "Le salarié pourra aussi demander des précisions sur les critères d'évolution salariale. Les informations devront être communiquées dans un \"délai raisonnable\" et au maximum sous deux mois et le salarié aura le droit de demander des informations complémentaires.\n",
        "\"\"\"\n",
        "#print('-', article,'\\n\\n')\n",
        "print(generate_response(prompt))\n",
        "\n",
        "\n",
        "# test the model on out-of-distribution prompt 2 :\n",
        "prompt = \"Do you know the reasons as to why people love coffee so much?\"\n",
        "print('\\n\\n\\n-', prompt, '\\n')\n",
        "print(generate_response(prompt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grNSwmt1omZ1"
      },
      "source": [
        "#### **Merging the main model with the adapter**\n",
        "\n",
        "After completing the fine-tuning process, our model consists of the original pre-trained weights and the LoRA adapters. Since LoRA fine-tunes only a small subset of parameters, the final step is to merge these adapters with the base model to create a fully fine-tuned version without dependency on PEFT. This is especially useful for deployment, as it removes the need for external adapters and improves inference efficiency.\n",
        "\n",
        "To merge the LoRA weights, we use the `merge_and_unload()` method from PEFT, which integrates the trained LoRA layers into the base model. Once merged, the model behaves as if it was fully fine-tuned, and we can save it for direct use without requiring PEFT or LoRA during inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1bo54C4pZMW",
        "outputId": "14355a07-9b88-4df9-eff2-90e658a68a41"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): Qwen2ForCausalLM(\n",
              "      (model): Qwen2Model(\n",
              "        (embed_tokens): Embedding(151936, 896)\n",
              "        (layers): ModuleList(\n",
              "          (0-23): 24 x Qwen2DecoderLayer(\n",
              "            (self_attn): Qwen2Attention(\n",
              "              (q_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=896, out_features=896, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=896, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=896, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=896, out_features=128, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=896, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=128, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=896, out_features=128, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=896, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=128, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=896, out_features=896, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=896, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=896, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "            )\n",
              "            (mlp): Qwen2MLP(\n",
              "              (gate_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=896, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=4864, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=896, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=4864, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4864, out_features=896, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4864, out_features=32, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=32, out_features=896, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "            (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "          )\n",
              "        )\n",
              "        (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "        (rotary_emb): Qwen2RotaryEmbedding()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model # check the model architecture with the added LoRA layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd5WZDWdoylU",
        "outputId": "fc98c74e-6489-45aa-bb55-5bfb557f8123"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "model = model.merge_and_unload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STgUbrVspb5P",
        "outputId": "bf64abe8-dd63-4c3e-fc64-65bed0338de5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Qwen2ForCausalLM(\n",
              "  (model): Qwen2Model(\n",
              "    (embed_tokens): Embedding(151936, 896)\n",
              "    (layers): ModuleList(\n",
              "      (0-23): 24 x Qwen2DecoderLayer(\n",
              "        (self_attn): Qwen2Attention(\n",
              "          (q_proj): Linear4bit(in_features=896, out_features=896, bias=True)\n",
              "          (k_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
              "          (v_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
              "          (o_proj): Linear4bit(in_features=896, out_features=896, bias=False)\n",
              "        )\n",
              "        (mlp): Qwen2MLP(\n",
              "          (gate_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=4864, out_features=896, bias=False)\n",
              "          (act_fn): SiLUActivation()\n",
              "        )\n",
              "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "      )\n",
              "    )\n",
              "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "    (rotary_emb): Qwen2RotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model # check the model architecture after merging."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d4gN4tTu47d"
      },
      "source": [
        "To go further:\n",
        "- Check **VLLM** for fast batch inference.\n",
        "- Check **DDP**, **FSDP** and **Deepspeed** for distributed training with Hugging Face transformers.\n",
        "- Check **unsloth** for faster training.\n",
        "- Check **ollama** for chatting interface.\n",
        "- Test **multi-turn** and **few-shot learning**.\n",
        "- Check **Megatron, Nanotron, etc..** for distributed **pre-training** on big clusters.\n",
        "- Check **LLama Factory** (https://github.com/hiyouga/LLaMA-Factory) for **Finetuning**."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "cgk3COo6QDq6",
        "z8Z2srf-7VTu"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
